{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction d'un système de recommandation\n",
    "\n",
    "Nous avons décidé d'orienter notre projet sur la recommendation de films.\n",
    "En effet durant ce confinement, nous avons eu le temps de visionner beaucoup de films,\n",
    "mais nous nous sommes rendus compte que nous passions quasiment autant de temps\n",
    "à choisir le film qu'à le regarder. D'où la nécessité de créer un système de re-\n",
    "commendations afin d'optimiser notre temps de visionnage.\n",
    "Nous avons chercher une base de données assez exploitable afin de mener à bien\n",
    "notre projet. Nous nous sommes basés sur la base de données de 'The Movies Dataset'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Différents systèmes de recommandation\n",
    "\n",
    "- [x] popularity based = moyenne simple\n",
    "- [x] memory-based (user- et item- based)\n",
    "- [x] hybride : popularity/collabo\n",
    "- [x] clustering\n",
    "- [ ] hybride : cluster/collabo\n",
    "- [ ] model-based (matrix factorisation, optimisation avec descente de gradient)\n",
    "    - [x] descente de gradient\n",
    "    - [ ] cross-validation pour tuner les hyperparamètres\n",
    "- [ ] hybride : cluster/model\n",
    "- [ ] user-centered linear approach = descente de gradient (même pb d'opti que model-based, mais on donne les infos des films)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "from time import time\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching and cleaning data\n",
    "\n",
    "Nous utilisons deux tables de données. L'une, *movies_metadata.csv*, contient une liste de films et des informations relativesau genre, date de sortie etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"movies_metadata.csv\")\n",
    "ratings = pd.read_csv(\"ratings_small.csv\")\n",
    "keywords = pd.read_csv(\"keywords.csv\")\n",
    "credits = pd.read_csv(\"tmdb_5000_credits.csv\")\n",
    "link = pd.read_csv(\"links_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 119050, 'name': 'Grumpy Old Men Collect...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 10749, 'name': 'Romance'}, {'id': 35, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15602</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>en</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>A family wedding reignites the ancient feud be...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
       "      <td>Grumpier Old Men</td>\n",
       "      <td>False</td>\n",
       "      <td>6.5</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31357</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>en</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Friends are the people who let you be yourself...</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>False</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 96871, 'name': 'Father of the Bride Col...</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11862</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>en</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>Just when George Banks has recovered from his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-02-10</td>\n",
       "      <td>76578911.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
       "      <td>Father of the Bride Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>5.7</td>\n",
       "      <td>173.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "2  False  {'id': 119050, 'name': 'Grumpy Old Men Collect...         0   \n",
       "3  False                                                NaN  16000000   \n",
       "4  False  {'id': 96871, 'name': 'Father of the Bride Col...         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "2  [{'id': 10749, 'name': 'Romance'}, {'id': 35, ...   \n",
       "3  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "4                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "\n",
       "                               homepage     id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story    862  tt0114709                en   \n",
       "1                                   NaN   8844  tt0113497                en   \n",
       "2                                   NaN  15602  tt0113228                en   \n",
       "3                                   NaN  31357  tt0114885                en   \n",
       "4                                   NaN  11862  tt0113041                en   \n",
       "\n",
       "                original_title  \\\n",
       "0                    Toy Story   \n",
       "1                      Jumanji   \n",
       "2             Grumpier Old Men   \n",
       "3            Waiting to Exhale   \n",
       "4  Father of the Bride Part II   \n",
       "\n",
       "                                            overview  ... release_date  \\\n",
       "0  Led by Woody, Andy's toys live happily in his ...  ...   1995-10-30   \n",
       "1  When siblings Judy and Peter discover an encha...  ...   1995-12-15   \n",
       "2  A family wedding reignites the ancient feud be...  ...   1995-12-22   \n",
       "3  Cheated on, mistreated and stepped on, the wom...  ...   1995-12-22   \n",
       "4  Just when George Banks has recovered from his ...  ...   1995-02-10   \n",
       "\n",
       "       revenue runtime                                   spoken_languages  \\\n",
       "0  373554033.0    81.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "1  262797249.0   104.0  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...   \n",
       "2          0.0   101.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "3   81452156.0   127.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "4   76578911.0   106.0           [{'iso_639_1': 'en', 'name': 'English'}]   \n",
       "\n",
       "     status                                            tagline  \\\n",
       "0  Released                                                NaN   \n",
       "1  Released          Roll the dice and unleash the excitement!   \n",
       "2  Released  Still Yelling. Still Fighting. Still Ready for...   \n",
       "3  Released  Friends are the people who let you be yourself...   \n",
       "4  Released  Just When His World Is Back To Normal... He's ...   \n",
       "\n",
       "                         title  video vote_average vote_count  \n",
       "0                    Toy Story  False          7.7     5415.0  \n",
       "1                      Jumanji  False          6.9     2413.0  \n",
       "2             Grumpier Old Men  False          6.5       92.0  \n",
       "3            Waiting to Exhale  False          6.1       34.0  \n",
       "4  Father of the Bride Part II  False          5.7      173.0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>862</td>\n",
       "      <td>[{'id': 931, 'name': 'jealousy'}, {'id': 4290,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8844</td>\n",
       "      <td>[{'id': 10090, 'name': 'board game'}, {'id': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>15602</td>\n",
       "      <td>[{'id': 1495, 'name': 'fishing'}, {'id': 12392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31357</td>\n",
       "      <td>[{'id': 818, 'name': 'based on novel'}, {'id':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11862</td>\n",
       "      <td>[{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           keywords\n",
       "0    862  [{'id': 931, 'name': 'jealousy'}, {'id': 4290,...\n",
       "1   8844  [{'id': 10090, 'name': 'board game'}, {'id': 1...\n",
       "2  15602  [{'id': 1495, 'name': 'fishing'}, {'id': 12392...\n",
       "3  31357  [{'id': 818, 'name': 'based on novel'}, {'id':...\n",
       "4  11862  [{'id': 1009, 'name': 'baby'}, {'id': 1599, 'n..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>[{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...</td>\n",
       "      <td>[{\"credit_id\": \"52fe48009251416c750aca23\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>285</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>[{\"cast_id\": 4, \"character\": \"Captain Jack Spa...</td>\n",
       "      <td>[{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>206647</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>[{\"cast_id\": 1, \"character\": \"James Bond\", \"cr...</td>\n",
       "      <td>[{\"credit_id\": \"54805967c3a36829b5002c41\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>49026</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>[{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba...</td>\n",
       "      <td>[{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49529</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>[{\"cast_id\": 5, \"character\": \"John Carter\", \"c...</td>\n",
       "      <td>[{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                     title  \\\n",
       "0     19995                                    Avatar   \n",
       "1       285  Pirates of the Caribbean: At World's End   \n",
       "2    206647                                   Spectre   \n",
       "3     49026                     The Dark Knight Rises   \n",
       "4     49529                               John Carter   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{\"cast_id\": 242, \"character\": \"Jake Sully\", \"...   \n",
       "1  [{\"cast_id\": 4, \"character\": \"Captain Jack Spa...   \n",
       "2  [{\"cast_id\": 1, \"character\": \"James Bond\", \"cr...   \n",
       "3  [{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba...   \n",
       "4  [{\"cast_id\": 5, \"character\": \"John Carter\", \"c...   \n",
       "\n",
       "                                                crew  \n",
       "0  [{\"credit_id\": \"52fe48009251416c750aca23\", \"de...  \n",
       "1  [{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de...  \n",
       "2  [{\"credit_id\": \"54805967c3a36829b5002c41\", \"de...  \n",
       "3  [{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de...  \n",
       "4  [{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_correct_id(word):\n",
    "    if not isinstance(word, str) or re.fullmatch(r'[0-9]+', word):\n",
    "        return word\n",
    "    return \"wrong_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_genre(l):\n",
    "    if len(l) <= 0 :\n",
    "        return []\n",
    "    if isinstance(l[0], dict):\n",
    "        return [d['name'] for d in l]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not re-run !\n",
    "\n",
    "movies = movies.drop_duplicates('id')\n",
    "keywords = keywords.drop_duplicates('id')\n",
    "credits = credits.drop_duplicates('movie_id')\n",
    "\n",
    "movies.id = movies.id.apply(filter_correct_id)\n",
    "movies = movies[movies.id != \"wrong_id\"]\n",
    "movies.id = movies.id.astype('int64')\n",
    "movies[movies['vote_count'].notnull()]['vote_count'].astype('int64', copy=False)\n",
    "movies[movies['vote_average'].notnull()]['vote_average'].astype('int64', copy=False)\n",
    "\n",
    "ratings = ratings.drop(columns=['timestamp'])\n",
    "\n",
    "movies = movies.rename(columns={'id' : 'tmdbId'})\n",
    "keywords = keywords.rename(columns={'id' : 'tmdbId'})\n",
    "credits = credits.rename(columns={'movie_id' : 'tmdbId'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans tout le notebook, on considère l'existence d'une variable globale `dfr` contenant la dataframe des notes et `dfm` contenant la dataframe des films. Cela nous permet d'abord tester notre code sur des petits échantillons et avant de les faire tourner sur la totalité des donneés, sans avoir à modifier le code. De même,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top films par genres\n",
    "\n",
    "Une méthode simple pour recommander des films à un utilisateur, est de lui présenter la liste des films les plus populaires appartenant à ses genres préférées. Ainsi il faudra dans un premier temps identifier ses genres préférés, puis établir la liste des films les plus populaires qui figurent parmi ces genres.\n",
    "\n",
    "Notre classs `PopularityRecommender` construit tout d'abord les tables qui vont nous être utiles. La dataframe `dfm` de schéma *dfm(title, vote_average, vote_count)* contient la liste des films identifiés par leur `movieId`. Pour pouvoir sélectionner les films par genre, et parce qu'un film peut appartenir à plusieurs genres, nous utlisont également une table `sgr` dans laquelle chaque ligne n'indique qu'un seul genre du film. Indexer ces deux tables par l'attribut `movieId` nous permet d'accélerer la sélection des lignes puisqu'elle ne se fait presque que par `movieId`.\n",
    "\n",
    "Enfin, la table `dfm` contenant les notes des utilisateurs et des films est utilisée pour déterminer les genres préféres d'un user donné : la méthode `pref_genres` selectionne les genres des 3 films préférés de l'utilisateurs. \n",
    "\n",
    "\n",
    "Pour évaluer la popularitée d'un film au sein d'un catégorie de films, nous utilisons la formule de *weighted rating* utilisée par le site TMDB : \n",
    "$$\n",
    "WR = \\frac{v}{v + m} R + \\frac{m}{v +m} C\n",
    "$$\n",
    "\n",
    "où \n",
    "- $R$ est la note moyenne du film (vote_average) ;\n",
    "\n",
    "- $v$ est le nombre de notes que le film a reçu (vote_count) ; \n",
    "\n",
    "- $m$ est le nombre minimum de votes qu'un film doit recevoir pour pouvoir figurer sur la liste ;\n",
    "\n",
    "- $C$ est la note moyenne pour tous les films.\n",
    "\n",
    "\n",
    "Si $R$, $v$ et $C$ se calculent à partir des données, il nous faut choisir le seuil $m$. Nous allons considérer qu'un film doit avoir eu plus de votes qu'au moins 80% des films pour pouvoir aparaître dans le top du genre. Ce paramètre permet de ne considérer que des films qui ont été vu par une majorité de personnes et qui peuvent être donc considérés comme populaire. \n",
    "\n",
    "La méthode `best` calcule donc ce critère de popularité pour les films appartenant à une liste de genres données et renvoie les meilleurs. Il suffit ensuite (dans la méthode `recommend`) de filtrer cette liste par les films non encore visionnés par l'utilisateur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rating(x, m, C):\n",
    "        '''\n",
    "        x[:, 0] correspond à vote_count\n",
    "        x[:, 1] correspond à vote average\n",
    "        '''\n",
    "        v = x[:, 0]\n",
    "        R = x[:, 1]\n",
    "        return np.multiply(v/(v+m), R) + np.multiply(m/(m+v), C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityRecommender:\n",
    "    def __init__(self, movies, ratings, link):\n",
    "        self.dfm = movies[['tmdbId', 'title', 'genres', 'vote_average', 'vote_count']]\n",
    "        self.dfm = link.merge(self.dfm, left_on='tmdbId', right_on='tmdbId').drop(columns=['tmdbId', 'imdbId'])\n",
    "        self.dfm.set_index('movieId', inplace=True)\n",
    "        self.dfm['genres'] = self.dfm['genres']\\\n",
    "                    .apply(lambda x: literal_eval(x) if isinstance(x, str) else x)\\\n",
    "                    .apply(simplify_genre)\n",
    "\n",
    "        self.dfr = ratings[['userId', 'movieId', 'rating']]\n",
    "        \n",
    "        self.sgr = self.dfm.apply(lambda x: pd.Series(x['genres'], dtype='str'),axis=1).stack().reset_index(level=1, drop=True)\n",
    "        self.sgr.name = 'genre'\n",
    "    \n",
    "        \n",
    "    def recommend(self, uid, n=20):\n",
    "        '''\n",
    "        Retourne les n films les plus populaires appartenant aux genres préféres de l'user uid\n",
    "        '''\n",
    "        chart = self._best_(self.pref_genres(uid), n*10)\n",
    "        watched_movies = (self.dfr).loc[self.dfr['userId'] == uid].movieId.unique()\n",
    "        watched_movies = chart.index.isin(watched_movies)\n",
    "        chart = chart.loc[~watched_movies]\n",
    "        return chart.head(n) if chart.shape[0] > n else chart\n",
    "    \n",
    "    def pref_genres(self, uid):\n",
    "        '''\n",
    "        Retourne les genres des 3 films préférés de l'user uid\n",
    "        '''\n",
    "        rats = self.dfr.loc[self.dfr['userId'] == uid].sort_values(by='rating')\n",
    "        pref = rats.head(3)['movieId'] if rats.shape[0] > 5 else rats['movieId']\n",
    "        genres = []\n",
    "        for g in self.dfm.loc[pref.values].genres :\n",
    "            genres = list(set(genres) | set(g))\n",
    "        return genres\n",
    "\n",
    "    def _best_(self, genres, k=200):\n",
    "        # select movies that are in genres\n",
    "        dfg = self.sgr[self.sgr.isin(genres)]\n",
    "        dfg = self.dfm.loc[dfg.index]\n",
    "        # need to drop duplicates : a movie can be selected because 2 or more of its genres are ok\n",
    "        dfg = dfg.loc[~dfg.index.duplicated()]\n",
    "\n",
    "        C = dfg.vote_average.mean()\n",
    "        m = dfg.vote_count.quantile(0.8)\n",
    "\n",
    "        top = dfg[(dfg['vote_count'] >= m)]\n",
    "        top['wr'] = weighted_rating(top[['vote_count', 'vote_average']].to_numpy(), m, C)\n",
    "        top.sort_values('wr', ascending=False, inplace=True)\n",
    "        top.drop(columns=['vote_count', 'vote_average'], inplace=True)\n",
    "        return top.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pop = PopularityRecommender(movies, ratings, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = PopularityRecommender(movies, ratings, link)\n",
    "print(pop.pref_genres(100))\n",
    "pop.recommend(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.loc[ratings['userId']==100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pop.pref_genres(4))\n",
    "pop.recommend(4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> À blablater : méthode pas personalisée, privilégie les plus populaires et ne permet pas d'évaluer de manière quantitative la pertinence (pas de note)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> À blablater : méthode pas personalisée, privilégie les plus populaires et ne permet pas d'évaluer de manière quantitative la pertinence (pas de note)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering : user- et item- based\n",
    "\n",
    "Pour prédire la note d'un couple (*user*, *movie*) on peut regarder quelle note les utilisateurs similaires à *user* ont donné à ce film et faire une moyenne de leurs notes. On peut également regarder quelle note *user* a donné à des films similaires à *movie*. La première approche est centré sur les utilisateurs, *user-based*, tandis que la deuxième est centrée sur les films, *item-based*. Neánsmoins les deux approches suivent la même logique et nous allons implémenter des fonctions qui s'adaptent en fonction de l'approche choisie. Dans un système *user-based*, nous allons appeler **peers** les **users** et **others** les **items**. Dans un système *item-based* c'est l'inverse.\n",
    "\n",
    "\n",
    "\n",
    "Deux utilisateurs sont considérés comme similaires s'ils ont les mêmes préférences de films. Il semble en effet plus pertinent de demander à un utilisateurs aux goûts similaires à *user* de lui conseiller un film. Pour comparer deux utilisateurs il faudra donc regarder les notes qu'ils ont donné aux mêmes films. De manière analogue, deux films sont similaires s'ils sont appréciés par les mêmes utilisateurs. Il faudra donc regarder les notes données par les mêmes utilisateurs pour comparer deux films. Cette notion de similitude sera calculée par un taux de corrélation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons considérer une variable globale `cm_user` et `cm_movie` contenant la matrice de correlation entre utilisateurs et films. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation des notes\n",
    "\n",
    "Nous n'avons besoin pour ce système que des notes données par les utilisateurs. Puisque la moyenne des notes données varie d'un utilisateur à un autre et d'un film à un autre, nous allons translater les notes afin que la moyenne des notes se trouve à 0. En *user-based*, on considère la moyenne par utilisateur, tandis qu'en *item-based* on s'interèsse à la moyenne par film. Par abus de langage nous appelons ces nouvelles notes les notes *normalisées*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la matrice de corrélation\n",
    "\n",
    "\n",
    "Dans un système *user-based*, on note $I_u$ l'ensemble des items renseignés pour l'utilisateur $u$ et $U_k$ l'ensemble des utilisateurs qui ont notés le film $k$. On note $I_{uv} = I_u \\cap I_v$. Pour le *item-based* on utilisera les mêmes notations en intervertissant user et item. On notera également $S_{ui}$ la note normalisée de l'item *i* donnée par l'utilisateur *u*. \n",
    "\n",
    "\n",
    "Pour déterminer si deux utilisateurs se ressemblent en termes de goûts, nous utilisons un taux de corrélation sur les avis données. Nous allons comparer quatres taux de corrélations différents. Le premier ```cor()``` calcule le taux de corrélation classique donné par la formule :\n",
    "$$\n",
    "cor(u, v) = \\frac{\\sum_{k \\in I_{uv}} s_{uk} s_{vk}}{\\sqrt{\\sum_{k \\in I_{uv}} s_{uk}^2}\\sqrt{\\sum_{k \\in I_{uv}} s_{vk}^2}}\n",
    "$$\n",
    "\n",
    "Le taux de corrélation ajusté ```cor_adj()``` permet de ne pas donner trop d'importance aux films populaires que beaucoup de personnes ont vu.\n",
    "$$\n",
    "cor\\_adj(u, v) = \\frac{\\sum_{k \\in I_{uv}} s_{uk} s_{vk} / U_k}{\\sqrt{\\sum_{k \\in I_{uv}} \\frac{s_{uk}^2}{|U_k|}}\\sqrt{\\sum_{k \\in I_{uv}} \\frac{s_{vk}^2}{|U_k|}}}\n",
    "$$\n",
    "\n",
    "Le taux de correlation calculé par ```cor_dis()``` permet de ne pas donner une correlation trop élevée si les deux utilisateurs n'ont pas donné assez d'avis sur des films en commun. \n",
    "$$\n",
    "cor\\_dis(u, v) = cor(u, v) * \\frac{min(|I_{uv}|, \\beta)}{\\beta}\n",
    "$$\n",
    "\n",
    "Enfin la fonction ```cor_dis_adj()``` fait un mélange des deux dernières amélioration : il filtre les films trop populaire et n'apporte de l'importance seulement si deux personnes ont données leur avis sur un certain nombre de films.\n",
    "\n",
    "$$\n",
    "cor\\_dis(u, v) = cor\\_adj(u, v) * \\frac{min(|I_{uv}|, \\beta)}{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous construisons maintenant la matrice de correlation. Puisqu'une telle matrice est symétrique, nous avons préféré utiliser une dataframe à deux entrées et ainsi ne stocker la corrélation pour un couple qu'une seule fois. En procédant comme tel, l'ordre dans lequel on désigne un couple peer-peer sera important. Pour faciliter l'accès, nous trions d'abord la datframe `df` pour que les peers soient pris dans l'ordre croissant des id. Ainsi les doubles indices de la dataframe construite auront tous la propriété que le premier indice est strictement inférieur au deuxième. Lors de l'accès à la correlation entre deux peers *u et v* il suffira de les ranger dans le bon ordre.\n",
    "\n",
    "La fonction de corrélation à utiliser peut être précisée en argument et par défaut la fonction choisie est la corrélation classique.\n",
    "\n",
    "Nous utilisons également le module logging pour suivre le déroulement du calcul. Celui-ci peut être très long en fonction de la taille des données et en rappelant tous les 10 peers qu\n",
    "\n",
    "Comme le calcul de la matrice peut-être très long, afin d'voir un suivi du déroulement du calcul, on utilise le module logging. Cette fonctionalité est désactivée par défaut. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction\n",
    "\n",
    "Pour prédire la note donnée par un utilisateur à un film, nous allons faire un moyenne des notes données pour les k peer les plus proches. Dans une approche user-based, on regarde donc les k plus proches utilisateurs, dans une approche item-based, les k films les plus proches. Les plus proches sont ceux qui ont une corrélation la plus élevée. On appelle **p** le peer et **o** l'élément other.\n",
    "\n",
    "La moyenne effectuée est pondérée par les coefficients de corrélations. On ajoute également la moyenne des notes de **p** pour retrouver une note non normalisée.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{um} = \\mu_u + \\frac{\\sum_{v \\in P_u(m)} s_{vm} \\cdot cor(u, v)}{\\sum_{v \\in P_u(m)} |cor(u,v)|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Que faire si sum_dow == 0 ? + prédire une note possible ?</span> \n",
    "\n",
    "# <span style=\"color:red\"> on donne quoi comme correlation si su et/ou sv est nul ? J'ai mis 0 par défaut mais bon ...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tx_cor(u, v, dfr, base):\n",
    "    '''\n",
    "    :param: u, v - les id des peers (user ou movie) à comparer\n",
    "            base - un indicateur dy type de recommandation utilisé : 'user' ou 'movie'\n",
    "    :return: le taux de corrélation classique entre u et v.\n",
    "    '''\n",
    "    Iu = dfr.loc[u, :]\n",
    "    Iv = dfr.loc[v, :]\n",
    "    Iuv = list(set(Iu.index) & set(Iv.index))\n",
    "    if not len(Iuv) : # l'intersection est vide\n",
    "        return float('nan')\n",
    "\n",
    "    su = Iu.loc[Iuv]\n",
    "    sv = Iv.loc[Iuv]\n",
    "    su = su['rating_norm_'+base].to_numpy()\n",
    "    sv = sv['rating_norm_'+base].to_numpy()\n",
    "    \n",
    "    up = np.dot(su, sv)\n",
    "    down = math.sqrt(np.dot(su, su) * np.dot(sv, sv))\n",
    "\n",
    "# default value to change\n",
    "    if up == 0 or down == 0:\n",
    "        return 0\n",
    "    return up / down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">À adapter encore</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_adj(u, v, df, Iuv):\n",
    "    nb_rat = df.loc[:, ['movieId', 'rating']].groupby(['movieId']).count()\n",
    "    \n",
    "    sum_up = 0\n",
    "    sum_down_u = 0\n",
    "    sum_down_v = 0\n",
    "    for movie in Iuv.movieId.unique() :\n",
    "        suk = df.loc[(df['userId'] == u) & (df['movieId'] == movie), ['rating_norm_'+base]]\n",
    "        svk = df.loc[(df['userId'] == v) & (df['movieId'] == movie), ['rating_norm_'+base]]\n",
    "        suk, svk = float(suk), float(svk)\n",
    "        \n",
    "        sum_up += suk * svk / nb_rat.at[movie, 'rating']\n",
    "        sum_down_u += suk**2 /  nb_rat.at[movie, 'rating']\n",
    "        sum_down_v += svk**2 /  nb_rat.at[movie, 'rating']\n",
    "    return sum_up / math.sqrt(sum_down_u * sum_down_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_dis(u, v, df, Iuv):\n",
    "    beta = 20\n",
    "    correlation = cor(u, v, df, Iuv)\n",
    "    return correlation * min(len(Iuv), beta)/beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_dis_adj(u, v, df, Iuv):\n",
    "    beta = 20\n",
    "    correlation = cor_adj(u, v, df, Iuv)\n",
    "    return correlation * min(len(Iuv), beta)/beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prédiction\n",
    "\n",
    "Pour prédire la note donnée par un utilisateur à un film, nous allons faire un moyenne des notes données pour les k peer les plus proches. Dans une approche user-based, on regarde donc les k plus proches utilisateurs, dans une approche item-based, les k films les plus proches. Les plus proches sont ceux qui ont une corrélation la plus élevée. On appelle **p** le peer et **o** l'élément other.\n",
    "\n",
    "La moyenne effectuée est pondérée par les coefficients de corrélations. On ajoute également la moyenne des notes de **p** pour retrouver une note non normalisée.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{um} = \\mu_u + \\frac{\\sum_{v \\in P_u(m)} s_{vm} \\cdot cor(u, v)}{\\sum_{v \\in P_u(m)} |cor(u,v)|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBasedPredictor:\n",
    "    def __init__(self, base):\n",
    "        assert base in {'user', 'movie'}\n",
    "        self.base = base\n",
    "        self.ptype, self.otype = ('userId', 'movieId') if self.base == 'user' else ('movieId', 'userId')\n",
    "        self.dfr = None\n",
    "        self.cm = None\n",
    "        self.id_index = None # explain what this is for\n",
    "        self.mean_peers = None\n",
    "    \n",
    "    def fit(self, ratings, cor_fct, verbose=False):\n",
    "        \n",
    "        assert 'userId' in ratings.columns and  'movieId' in ratings.columns\n",
    "        \n",
    "        self.dfr = ratings[['userId', 'movieId', 'rating']].set_index([self.ptype, self.otype])\n",
    "        self.dfr.sort_index(inplace=True)\n",
    "\n",
    "        # normalize ratings per peer\n",
    "        self.mean_peers = self._normalize_()\n",
    "        \n",
    "        # construction of the correlation matrix\n",
    "        self.cm, self.id_index = self._CorMatrix_(cor_fct, verbose)\n",
    "\n",
    "    def predict(self, p, o, k=4):\n",
    "        '''\n",
    "        Retourne la prédiction de la note du couple (p, o) utilisant un peer-groupe de taille k\n",
    "        '''\n",
    "        assert self.dfr is not None, \"This MemoryBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        peer_unknown = p not in self.mean_peers\n",
    "        other_unknown = o not in self.dfr.index.droplevel(self.ptype)\n",
    "        \n",
    "        if peer_unknown and other_unknown: # si peer et/ou other inconnu, prédire des valeurs arbitraire\n",
    "            return 2.5\n",
    "        elif peer_unknown:\n",
    "            return float(self.dfr.loc[(slice(None), o), 'rating'].mean())\n",
    "        elif other_unknown:\n",
    "            return self.mean_peers[p]\n",
    "        \n",
    "        mu = self.mean_peers[p]\n",
    "        peers = self._PeerGroup_(p, o, k)\n",
    "\n",
    "        sum_up, sum_down = 0, 0\n",
    "        for friend in peers:\n",
    "            cor = self.get_cor(p, friend)\n",
    "            if not math.isnan(cor):\n",
    "                sfo = self.dfr.loc[(friend, o), 'rating_norm_'+self.base]\n",
    "                sfo = 0 if len(sfo) <= 0 else float(sfo)\n",
    "                sum_up += sfo * cor\n",
    "                sum_down += abs(cor)\n",
    "\n",
    "        sum_down = 1 if sum_down == 0 else sum_down\n",
    "        pred = mu + sum_up / sum_down\n",
    "\n",
    "        # note prédite à une précision de 0.5\n",
    "        pred = round(2 * pred) / 2\n",
    "        # note prédite se trouve entre 0 et 5\n",
    "        pred = 5 if pred > 5 else pred\n",
    "        pred = 0 if pred < 0 else pred\n",
    "\n",
    "        return pred\n",
    "        \n",
    "    def score(self, test_ratings, k=4):\n",
    "        assert self.dfr is not None, \"This MemoryBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        dft = test_ratings.set_index([self.ptype, self.otype])\n",
    "        predictions = [self.predict(p, o, k) for (p, o) in list(dft.index)]\n",
    "        truth = dft['rating'].to_numpy()\n",
    "        \n",
    "        return 1 - math.sqrt(sum((predictions - truth)**2) / len(predictions)) / 5\n",
    "    \n",
    "    \n",
    "    def get_cor(self, u, v):\n",
    "        '''\n",
    "        Retourne le taux de correlation entre u et v\n",
    "        '''\n",
    "        if u in self.id_index and v in self.id_index:\n",
    "            ixu, ivx = self.id_index[u], self.id_index[v]\n",
    "            return self.cm[ixu, ixv]\n",
    "        return float('nan')\n",
    "    \n",
    "    def _normalize_(self):\n",
    "        '''\n",
    "        Ajoute une colonne dans la dataframe df contenant les notes normalisées des utilisateurs\n",
    "        Retourne la Série donnant la moyenne des notes par peer\n",
    "        '''\n",
    "        mean = self.dfr.groupby(self.ptype).mean()['rating']\n",
    "        new_col = 'rating_norm_'+self.base\n",
    "        self.dfr[new_col] = self.dfr.apply(lambda row : row[0]- mean[row.name[0]] , axis=1) \n",
    "        return mean\n",
    "    \n",
    "    def _CorMatrix_(self, cor_fct, verbose=False):\n",
    "        '''\n",
    "        Retourne la matrice de corrélation entre peers\n",
    "        '''\n",
    "        peers = self.dfr.index.get_level_values(self.ptype).unique()\n",
    "        nb_peers = len(peers)\n",
    "\n",
    "        if verbose:\n",
    "            logger = logging.getLogger()\n",
    "            logger.setLevel(logging.INFO)\n",
    "            logging.info('nb of peers: {}'.format(nb_peers))\n",
    "\n",
    "        cor_mat = np.empty((nb_peers,nb_peers))\n",
    "        cor_mat[:] = np.nan\n",
    "\n",
    "        for i in range(nb_peers):\n",
    "            u = peers[i]\n",
    "            if verbose and not i % 10 : \n",
    "                logging.info('peer nb: {} (id {})'.format(i, u))\n",
    "            for j in range(i + 1, nb_peers):\n",
    "                v = peers[j]\n",
    "                tx_cor = cor_fct(u, v, self.dfr, self.base)\n",
    "                if not np.isnan(tx_cor):\n",
    "                    cor_mat[i, j] = tx_cor\n",
    "                    cor_mat[j, i] = cor_mat[i, j]\n",
    "        peer_rank = {}\n",
    "        for pid in peers:\n",
    "            peer_rank[pid] = len(peer_rank)\n",
    "        \n",
    "        return cor_mat, peer_rank\n",
    "    \n",
    "    def _PeerGroup_(self, p, o, k=4):\n",
    "        '''\n",
    "        Retourne les k peers les plus proches de p\n",
    "        '''\n",
    "        # in user-based, get users that rated the movie o\n",
    "        peers = self.dfr.loc[(slice(None), o), :]\n",
    "        peers = peers.index.get_level_values(self.ptype).unique()\n",
    "        \n",
    "        top = [(float('-inf'), p)] * k\n",
    "        for v in peers:\n",
    "            taux = self.get_cor(p, v)\n",
    "            if taux > top[-1][0] :\n",
    "                top += [(taux, v)]\n",
    "                top.sort(reverse=True)\n",
    "                top = top[:-1]\n",
    "        return [t[1] for t in top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle hybride : popularité par genre et collaborative filtering\n",
    "\n",
    "La méthode précédente permet de prédire une note, mais nous aimerions pouvoir recommander des films à un utilisateurs qu'il est susceptible d'aimer. Pour cela il faudrait prédire la note qu'il donnerait à tous les films qu'il n'a pas encore noté et prélever ceux dont la note prédite est la plus élevée. Ceci serait beaucoup trop coûteux. Une première solution est de ne considérer que des films appartenant à ses genres préférés. Ceci risque d'être toujours trop coûteux, alors nous allons nous restreindre qu'aux films les plus populaires dans ses genres préférés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryPopularityRecommender:\n",
    "    def __init__(self, base):\n",
    "        self.memory = MemoryBasedPredictor(base)\n",
    "        self.popularity = None\n",
    "    \n",
    "    def fit(self, movies, ratings, link, cor_fct, verbose=False):\n",
    "        self.popularity = PopularityRecommender(movies, ratings, link)\n",
    "        \n",
    "        self.memory.fit(ratings, cor_fct, verbose)\n",
    "    \n",
    "    def recommend(self, uid, nb_reco=20, k=4):\n",
    "        dfm_filtered = self.popularity.recommend(uid, nb_reco*10)\n",
    "        \n",
    "        predictions = pd.DataFrame(columns=['movieId', 'predict_rating'])\n",
    "        for mid in dfm_filtered.index.unique():\n",
    "            p, o = (uid, mid) if self.memory.base == 'user' else (mid, uid)\n",
    "            rat = (self.memory).predict(p, o, k)\n",
    "            predictions = predictions.append({'movieId':int(mid), 'predict_rating':rat}, ignore_index=True)\n",
    "        \n",
    "        predictions.sort_values(by='predict_rating', ascending=False, inplace=True)\n",
    "        reco = dfm_filtered.join(predictions.set_index('movieId'))\n",
    "        reco = reco.head(nb_reco) if reco.shape[0] > nb_reco else reco\n",
    "        return reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 4\n",
    "\n",
    "#hybride_genre(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette solution présente néanmoins un désavantage. Premièrement, seuls les films les plus populaires sont considérés, leur donnant plus de visibilté parmis les utilisateurs. Ainsi un nouveau film qui n'aura pas beaucoup été vu aura que peut de chance d'être recommandé par assez populaire. Le deuxième problème est que cette méthode regroupe les films par genres. Or ce qui caractérise un film est plus vaste que seulement la case dans laquelle il s'inscrit et peut dépendre du réalisateur, du lieu de tournage ou de pleins d'autres facteurs. C'est ici que le *clustering* nous vient en aide. Cela permet de regrouper les films selon leurs similitudes issues de méta-informations et ainsi d'affiner la recherche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering des films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage de la base de données et réduction de la matrice aux caractéristiques interéssantes\n",
    "\n",
    "Suppression des id incorrects, des valeurs abérrantes, des lignes avec NaN, et modification des valeurs pour les rendre plus faciles à traiter.\n",
    "\n",
    "On sélectionne les attributs de films qui semblent pertinents pour différencier les films sur leur contenu.\n",
    "Ces choix sont arbitraires et on pourra être amenés à réfléchir dessus et à les modifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne voulons garder que les films ayant reçu une note. Cela est une manière de ne garder qu'un nombre limité de films (il est très compliqué pour nous d'effectuer des calculs pour 45 000 films). De plus le clustering est intéressant pour renforcer la recommendation \"user-based\". On ne garde donc que les films ayant été notés par les utilisateurs. Ensuite on rajoute l'attribut keywords aux films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_cluster = movies.join(link.set_index('tmdbId'), on='tmdbId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_cluster = dfm_cluster.merge(ratings.drop_duplicates('movieId'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_cluster = dfm_cluster.join(keywords.set_index('tmdbId'), on='tmdbId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = dfm_cluster[['tmdbId','movieId','title']]\n",
    "cluster_features = dfm_cluster[['title', 'genres', 'keywords', 'release_date', 'production_countries', 'original_language', 'runtime']]\n",
    "cluster_features.index = dfm_cluster.movieId.apply(lambda x: str(x))\n",
    "cluster_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cluster_features.runtime.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit de ne retenir que les films d'une durée comprise entre 40 minutes et 4 heures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_runtime(dfm, inf=40, sup=240):\n",
    "    dfm = dfm[dfm.runtime >= inf]\n",
    "    dfm = dfm[dfm.runtime <= sup]\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = clean_runtime(cluster_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde la proportion de films pour lesquels certains champs n'ont pas été renseignés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de films retenus dans cluster_features : \", len(cluster_features))\n",
    "print(\"Parmi ces films :\")\n",
    "print(len(cluster_features[cluster_features.genres == \"[]\"]), \"n'ont pas de genres\")\n",
    "print(len(cluster_features[cluster_features.keywords == \"[]\"]), \"n'ont pas de keywords\")\n",
    "print(len(cluster_features[cluster_features.production_countries == \"[]\"]), \"n'ont pas de production_countries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'une petite proportion, on peut donc retirer ces films problématiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_values(dfm):\n",
    "    dfm = dfm[dfm.genres != \"[]\"]\n",
    "    dfm = dfm[dfm.keywords != \"[]\"]\n",
    "    dfm = dfm[dfm.production_countries != \"[]\"]\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = drop_missing_values(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de films dans cluster_features : \", len(cluster_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant se concentrer sur le traitement des données de chacune des colonnes. Il faut les simplifier au maximum pour rendre possible la comparaison de films basée sur ces attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_genres(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'genres'.\n",
    "    It turns the column genres from a string that contains a dictionnary into an int list of genres id.'''\n",
    "    \n",
    "    def genres_to_id(gen):\n",
    "        if isinstance(gen, str):\n",
    "            pattern = re.compile(r\"'id': [0-9]*\")\n",
    "            return np.array([int(w[6:]) for w in pattern.findall(gen)])\n",
    "        return gen\n",
    "    \n",
    "    dfm.genres = dfm.genres.apply(genres_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_genres(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_keywords(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'keywords'.\n",
    "    It turns the column keywords from a string that contains a dictionnary into an int list of keywords id.'''\n",
    "    \n",
    "    def keywords_to_id(kw):\n",
    "        if isinstance(kw, str):\n",
    "            pattern = re.compile(r\"'id': [0-9]*\")\n",
    "            return np.array([int(w[6:]) for w in pattern.findall(kw)])\n",
    "        return kw\n",
    "    \n",
    "    dfm.keywords = dfm.keywords.apply(keywords_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_keywords(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_date(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'release_date'.\n",
    "    It turns the column release_date from a string date into an int being the year the film was released.'''\n",
    "    \n",
    "    def date_to_int(date):\n",
    "        if isinstance(date, str):\n",
    "            return int(date[:4])\n",
    "        return date\n",
    "    \n",
    "    dfm.release_date = dfm.release_date.apply(date_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_date(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_countries(dfm):\n",
    "    \n",
    "    '''This function takes a DataFrame dfm that must contain a column 'production_countries'.\n",
    "    It turns the column production_countries from a string that contains a dictionnary into an int list of keywords id.'''\n",
    "    \n",
    "    def simplify(country):\n",
    "        if isinstance(country, str):\n",
    "            pattern = re.compile(r\"'iso_3166_1': ...\")\n",
    "            return [w[15:] for w in pattern.findall(country)]\n",
    "        return country\n",
    "    \n",
    "    dfm.production_countries = dfm.production_countries.apply(simplify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify_countries(cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition d'une distance sur les films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va réaliser plus bas un hierarchical agglomerative clustering. Le principe est simple : on commence avec n clusters de 1 film, puis on fusionne à chaque itération les 2 clusters les plus proches, jusqu'à n'avoir plus d'un seul cluster contenant tous les films. Cela requiert une distance sur les films. C'est ce qu'on va construire ici. La tâche n'est pas simple : chaque film a été réduit à 7 attributs, et il faut aggréger ces 7 attributs pour déterminer à quel point 2 films sont similaires. On peut choisir d'accorder un poids différent à chacun des critères en fonction de leur importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit ici des variables globales qui seront utilisées plus loin dans la fonction movie_distance\n",
    "MAX_YEAR_DIFFERENCE = max(cluster_features.release_date) - min(cluster_features.release_date)\n",
    "MAX_RUNTIME_DIFFERENCE = max(cluster_features.runtime) - min(cluster_features.runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction movie_distance calcule une distance entre 2 films. Plus cette valeur est proches de 0 et plus les films sont similaires. Plus la valeur est grande et plus ils sont différents. IMPORTANT : la built-in magic command %lprun nous a permis d'analyser la répartition du temps d'exécution lors du clustering sur les données. 99% du temps de calcul réside dans cette fonction de distance. Ce qui est le plus coûteux en temps est l'accès aux attributs des films. On les a donc réduit au strict minimum. De plus, on ne créé pas de vecteur à 7 coefficients qui stockerait la similitude entre les 2 films pour chaque critère. A la place, on additionne directement le carré de ces valeurs dans une variable de somme, puis on renvoit la racine carrée de cette variable. On utilise la norme 2 en la calculant à la main pour accélérer les calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_distance(movie1, movie2, w_gen=4, w_key=2, w_rel=2, w_pro=3, w_ori=2, w_run=1):\n",
    "    \n",
    "    '''This function computes the distance between 2 movies m1 and m2 given some weight parameters.\n",
    "    It can be called either with 2 lists of attributes or with 2 movie Series, but the 2 parameters\n",
    "    must have the same type.'''\n",
    "    \n",
    "    assert type(movie1) is type(movie2)\n",
    "    sum_vect = 0 # avoiding to store a vector just to compute his norm afterwards\n",
    "    \n",
    "    if isinstance(movie1, pd.Series):\n",
    "        g1, g2 = movie1.genres, movie2.genres\n",
    "        kw1, kw2 = movie1.keywords, movie2.keywords\n",
    "        r1, r2 = movie1.release_date, movie2.release_date\n",
    "        pc1, pc2 = movie1.production_countries, movie2.production_countries\n",
    "        lang1, lang2 = movie1.original_language, movie2.original_language\n",
    "        run1, run2 = movie1.runtime, movie2.runtime\n",
    "    else:\n",
    "        # Access to both movie's attributes stored in the lists movie1 and movie2\n",
    "        g1, g2 = movie1[0], movie2[0]\n",
    "        kw1, kw2 = movie1[1], movie2[1]\n",
    "        r1, r2 = movie1[2], movie2[2]\n",
    "        pc1, pc2 = movie1[3], movie2[3]\n",
    "        lang1, lang2 = movie1[4], movie2[4]\n",
    "        run1, run2 = movie1[5], movie2[5]\n",
    "    \n",
    "    # SIMILARITIES IN GENRES\n",
    "    gen = np.append(g1, g2)\n",
    "    sum_vect += w_gen * (1 - (len(gen) - len(np.unique(gen))) / min(len(g1), len(g2))) ** 2\n",
    "    \n",
    "    # SIMILARITIES IN KEYWORDS\n",
    "    kw = np.append(kw1, kw2)\n",
    "    # Having one keyword in common is sufficient to make 2 films similar for this criterion\n",
    "    # This choice was made because most films have many keywords\n",
    "    if len(kw) == len(np.unique(kw)):\n",
    "        sum_vect += w_key * 1 # ** 2\n",
    "    \n",
    "    # SIMILARITIES FOR THE RELEASE DATE\n",
    "    # the normalized difference between the 2 releade dates\n",
    "    sum_vect += w_rel * (abs(r1 - r2) / MAX_YEAR_DIFFERENCE) ** 2\n",
    "    \n",
    "    # SIMILARITIES IN PRODUCTION COUNTRIES\n",
    "    pc = []\n",
    "    pc.extend(pc1)\n",
    "    pc.extend(pc2)\n",
    "    pc_dist = 1 - (len(pc) - len(np.unique(pc))) / min(len(pc1), len(pc2))\n",
    "    # As it is rare, we set that 2 films which are not from the US have something in common\n",
    "    if 'US' not in pc1 and 'US' not in pc2 and pc_dist > 0.5:\n",
    "        sum_vect += w_pro * 0.5 ** 2\n",
    "    else:\n",
    "        sum_vect += w_pro * pc_dist ** 2\n",
    "    \n",
    "    # SIMILARITIES FOR THE LANGUAGE\n",
    "    if lang1 != lang2 :\n",
    "        # As well, 2 films whose original language is not english have something in common\n",
    "        if lang1 != 'en' and lang2 != 'en':\n",
    "            sum_vect += w_ori * 0.5 ** 2\n",
    "        else:\n",
    "            sum_vect += w_ori * 1 ** 2\n",
    "    \n",
    "    # SIMILARITIES FOR THE RUNTIME\n",
    "    # the normalized difference between the 2 runtimes\n",
    "    sum_vect += w_run * (abs(run1 - run2) / MAX_RUNTIME_DIFFERENCE) ** 2\n",
    "    \n",
    "    return np.sqrt(sum_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant qu'on dispose d'une distance entre les films, on doit calculer la matrice de distance entre les films. Pour cela, on va utiliser un DataFrame avec en index et columns les id des movies (en string pour éviter toute confusion avec loc et iloc). On initilise un DataFrame vide avec les bonnes dimensions et les bons index / columns. On le remplit ensuite en faisant appel à la fonction movie_distance pour chaque paire de films. Comme par la suite on veut chercher le coefficient minimum de cette matrice, on met la distance d'un film à lui même à 1000 - un nombre suffisemment grand pour qu'aucune autre valeur de distance ne l'approche avec les choix de poids qu'on a fait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_matrix(clu_fea):\n",
    "    \n",
    "    '''This function computes the distance matrix between all the movies contained in clu_fea.\n",
    "    The clu_fea DataFrame must have been cleaned before. Returns the distance matrix.'''\n",
    "    \n",
    "    movies_id = clu_fea.index\n",
    "    dist_mat = pd.DataFrame(np.nan * len(clu_fea), index=movies_id, columns=movies_id)\n",
    "    \n",
    "    # Faire des .loc / .iloc sur un DataFrame prend enormement de temps. Pour eviter cela,\n",
    "    # on le fait pour chaque attribut de chaque film une bonne fois pour toutes, et on stocke\n",
    "    # cela dans une matrice. On est forcé d'utiliser une double liste python car chaque element\n",
    "    # de la matrice peut etre de type int, float, str ou encore list. C'est envrion 4 fois plus\n",
    "    # rapide que de rester avec un DataFrame en accédant aux attributs des films !\n",
    "    \n",
    "    mat_mem = []\n",
    "    for i in range(len(clu_fea)):\n",
    "        mov_i = clu_fea.iloc[i]\n",
    "        mat_mem.append([mov_i.genres, mov_i.keywords, mov_i.release_date,\n",
    "                     mov_i.production_countries, mov_i.original_language, mov_i.runtime])\n",
    "    \n",
    "    # On peut maintenant calculer la distance entre chaque paire de films\n",
    "    # La distance d'un film avec lui meme est fixee a 1000 par defaut\n",
    "    # pour eviter de fusionner un cluster avec lui meme\n",
    "    \n",
    "    for i in range(len(clu_fea)):\n",
    "        for j in range(i, len(clu_fea)):\n",
    "            if i == j:\n",
    "                dist_mat.iat[i, j] = 1000\n",
    "            else:\n",
    "                dist_mat.iat[i, j] = dist_mat.iat[j, i] = movie_distance(mat_mem[i], mat_mem[j])\n",
    "    \n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HIERARCHICAL AGGLOMERATIVE CLUSTERING </b> <br>\n",
    "Dans ce type de méthode de clustering, on n'a pas besoin de préciser le nombre de clusters attendus. L'algorithme permet de construire un dendrogramme, et on obtient nos clusters en coupant le dendrogramme à une certaine hauteur. On va commencer par écrire une classe Dendrogram. Un objet de cette classe contient plusieurs attributs :\n",
    "<li>Un champ leaf - valant None pour les noeuds dans l'arbre et contenant l'id d'un film (int) pour les feuilles </li>\n",
    "<li>Un champ leaf_nb - un int indiquant pour chaque noeud le nombre de feuilles (et donc de films) de l'abre issu de ce noeud </li>\n",
    "<li>Un champ father - une référence vers le père du noeud </li>\n",
    "<li>Un champ left et un champ right - une référence vers le fils gauche (resp. fils droit) du noeud </li>\n",
    "<li>Un champ height - un float indiquant la hauteur du noeud. Attention ! Il ne s'agit pas de la notion classique de hauteur d'un noeud dans un arbre binaire. <br>Il s'agit ici de la hauteur de fusion entre les 2 groupes de films (fils gauche et fils droit). Plus elle est élevée et plus ces 2 groupes de films sont différents </li>\n",
    "<li>Un champ distance_to_father - un float indiquant la longueur de l'arête reliant le noeud à son père </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dendrogram:\n",
    "    \n",
    "    def __init__(self, leaf=None):\n",
    "        \n",
    "        '''This is the Dendrogram class constructor. It takes only one optional argument, which is leaf if the user\n",
    "        wants to build a leaf containing a movie id. Otherwise it is a node and the leaf attribute is set to None.\n",
    "        The other attributes are set to 0 or None for now, and should be modified by setters later on.'''\n",
    "        \n",
    "        self.leaf = leaf\n",
    "        self.leaf_nb = 1\n",
    "        self.father = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.height = 0\n",
    "        self.distance_to_father = 0\n",
    "    \n",
    "    def set_leaf_nb(self):\n",
    "        \n",
    "        '''This method is a setter for the leaf_nb attribute. It requires the left and right sons, as the total number\n",
    "        of leaves of the node is the sum of its left and right leaf_nb attributes. If called on a leaf, leaf_nb should\n",
    "        equal 1 due to the constructor, and set_leaf_nb set leaf_nb to 1 as well. This methode should always be called\n",
    "        after creating a node that is not a leaf.'''\n",
    "        \n",
    "        total_leaf_nb = 0\n",
    "        if self.left is not None:\n",
    "            total_leaf_nb += self.left.leaf_nb\n",
    "        if self.right is not None:\n",
    "            total_leaf_nb += self.right.leaf_nb\n",
    "        self.leaf_nb = max(1, total_leaf_nb)\n",
    "    \n",
    "    def set_height(self, height):\n",
    "        \n",
    "        '''This method is a setter for the height attribute. The height is given as a parameter. This method also sets\n",
    "        the distance_to_father attribute of the node's left and right sons. If called on a leaf, this method does nothing.\n",
    "        This methode should always be called after creating a node that is not a leaf.'''\n",
    "        \n",
    "        if self.left is None or self.right is None:\n",
    "            return\n",
    "        self.height = height\n",
    "        self.left.distance_to_father = height - self.left.height\n",
    "        self.right.distance_to_father = height - self.right.height\n",
    "    \n",
    "    def get_id_list(self):\n",
    "        \n",
    "        '''This method returns a list of all the id contained in the leafs of the node.It uses a prefix run.'''\n",
    "        \n",
    "        id_list = []\n",
    "        def prefix(node):\n",
    "            if node.leaf is not None:\n",
    "                id_list.append(node.leaf)\n",
    "            else:\n",
    "                prefix(node.right)\n",
    "                prefix(node.left)\n",
    "        prefix(self)\n",
    "        \n",
    "        return id_list\n",
    "    \n",
    "    def get_root(self):\n",
    "        \n",
    "        '''This method returns the root of the tree to which the node belongs. The main goal of this method is\n",
    "        to start from a leaf and find the root of the dendrogram, which is the leaf's cluster at a step k.'''\n",
    "        \n",
    "        tmp = self\n",
    "        while tmp.father is not None: tmp = tmp.father\n",
    "        return tmp\n",
    "    \n",
    "    def cut_at_threshold(self, threshold):\n",
    "        \n",
    "        '''This method provides a cut of the dendrogram at a height given by parameter threshold. It returns a list\n",
    "        of Dendrogram objects. Each element of the list can be seen as the root of a dendrogram, i.e. a cluster.'''\n",
    "        \n",
    "        assert threshold >= 0\n",
    "        assert threshold <= self.height\n",
    "        node_list = []\n",
    "        def step(node, t):\n",
    "            if node.height == 0:\n",
    "                node_list.append([node])\n",
    "            else:\n",
    "                if node.left.height <= t:\n",
    "                    node_list.append(node.left)\n",
    "                else:\n",
    "                    step(node.left, t)\n",
    "                if node.right.height <= t:\n",
    "                    node_list.append(node.right)\n",
    "                else:\n",
    "                    step(node.right, t)\n",
    "        step(self, threshold)\n",
    "        \n",
    "        return node_list\n",
    "    \n",
    "    def clusters_threshold_cut(self, threshold):\n",
    "        \n",
    "        '''This method uses cut_at_threshold in order to return a list of movies id list. Each element\n",
    "        of the list is a cluster that directly contains the id of all movies that belong to the cluster.'''\n",
    "        \n",
    "        cluster_list = self.cut_at_threshold(threshold)\n",
    "        return [node.get_id_list() for node in cluster_list]\n",
    "    \n",
    "    def find_best_cut(self):\n",
    "        \n",
    "        '''This method does dendrogram cuts at 200 different threshold and keeps the best cut. It returns a list of\n",
    "        movies id list, i.e. the different clusters. Some changes had to be made to avoid getting too many clusters.\n",
    "        This is due to the distance choice between movies. Some attributes such as director or keywords are almost\n",
    "        all different for 2 movies, resulting in a high distance between most movies, even between 2 movies that are\n",
    "        very similar regarding genres, language, release date, production countries and runtime. That is why the method\n",
    "        imposes a maximum number of clusters, which depends on the size of the input. The upper bound is quite high so\n",
    "        that it enables many clusters, and it avoids to have some situations with e.g. 100 movies and 43 clusters.'''\n",
    "        \n",
    "        max_clu_nb = self.leaf_nb / np.sqrt(self.leaf_nb)\n",
    "        threshold_list = np.linspace(0.01, self.height, 200)\n",
    "        def step(t):\n",
    "            nodes = self.cut_at_threshold(t)\n",
    "            if len(nodes) > max_clu_nb:\n",
    "                return 0\n",
    "            return sum([n.distance_to_father for n in nodes]) / len(nodes)\n",
    "        score = [step(t) for t in threshold_list]\n",
    "        best_threshold_index = np.argmax(score)\n",
    "        best_threshold = threshold_list[best_threshold_index]\n",
    "        \n",
    "        return self.clusters_threshold_cut(best_threshold)\n",
    "    \n",
    "    def get_n_clusters(self, n):\n",
    "        \n",
    "        '''This method provides a cut on the dendrogram that gives a number n of clusters, chosen in parameter.\n",
    "        The different clusters are obtained by cutting at a threshold that leads to n clusters.\n",
    "        It uses dichotomy in order to be efficient. It tries a cut at mid-height, check the number of\n",
    "        clusters obtained and then decides to stop, cut at a higher height or cut at a lower height.'''\n",
    "        \n",
    "        assert n >= 1\n",
    "        assert n <= self.leaf_nb\n",
    "        # dichotomy\n",
    "        xmin, xmax, xmid = 0, self.height, self.height / 2\n",
    "        nodes = self.cut_at_threshold(xmid)\n",
    "        while len(nodes) != n:\n",
    "            if len(nodes) < n: \n",
    "                xmax, xmid = xmid, (xmid + xmin) / 2\n",
    "            else:\n",
    "                xmin, xmid = xmid, (xmid + xmax) / 2\n",
    "            nodes = self.cut_at_threshold(xmid)\n",
    "            # if the loop does not end - extremely unlickely but not impossible if 2 nodes\n",
    "            # have the same height - it calls another cut function that returns n clusters\n",
    "            if xmax - xmin < 1e-5:\n",
    "                return self.get_n_clusters_perso(n)[0]\n",
    "        \n",
    "        return [n.get_id_list() for n in nodes]\n",
    "        \n",
    "    def get_n_clusters_perso(self, n):\n",
    "        \n",
    "        '''This method provides a cut on the dendrogram that gives a number n of clusters, chosen in parameter.\n",
    "        It is called get_n_clusters_perso because I imaginated it alone and I don't think it exists elsewhere.\n",
    "        The goal of this method is to solve the following problem : most of the time, cutting a dendrogram at\n",
    "        a given height leads to a certain number of clusters. Among these clusters, some can be very small, there\n",
    "        are even clusters with one element. So, instead of cutting at a threshold, the idea of this method is the\n",
    "        following : it starts at the root and must provide n clusters, e.g. let's take n=10. If the left son's leaf\n",
    "        number is greater than the right son's one, then the left son must provide let's say n=7 clusters, while the\n",
    "        right son must provide n=3 clusters. Finally if the right son's have a very small number of leaves compared\n",
    "        to the left son's one, then the left son must provide n=10 clusters and the leaves of the right son are added\n",
    "        to a list of outliers. At the end of the process, there will be 10 clusters quite well balanced and one list\n",
    "        of outliers, which can form a special cluster of, let's say, unclassifiable movies.'''\n",
    "        \n",
    "        if n <= 0 or n > self.leaf_nb:\n",
    "            print(\"Bad choice for n : too big or <= 0\")\n",
    "            return\n",
    "        cluster_list = []\n",
    "        outliers = []\n",
    "        error = []\n",
    "        def step(node, n):\n",
    "            if n == 1:\n",
    "                cluster_list.append(node.get_id_list())\n",
    "            elif node.left is None or node.right is None:\n",
    "                error.append(True)\n",
    "            else:\n",
    "                prop_left = node.left.leaf_nb / node.leaf_nb\n",
    "                prop_right = node.right.leaf_nb / node.leaf_nb\n",
    "                # a node is considered an outlier if his leaf number\n",
    "                # is less than 15% of his father's leaf number\n",
    "                if prop_left < 0.15:\n",
    "                    outliers.extend(node.left.get_id_list())\n",
    "                    step(node.right, n)\n",
    "                elif prop_right < 0.15:\n",
    "                    outliers.extend(node.right.get_id_list())\n",
    "                    step(node.left, n)\n",
    "                else:\n",
    "                    n_left = max(1, round(n * prop_left))\n",
    "                    if n_left == n:\n",
    "                        n_left -= 1\n",
    "                    n_right = n - n_left\n",
    "                    step(node.left, n_left)\n",
    "                    step(node.right, n_right)\n",
    "        step(self, n)\n",
    "        if error:\n",
    "            print(\"n too big\")\n",
    "        else:\n",
    "            return cluster_list, outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut désormais écrire une classe pour implémenter le Hierarchical Agglomerative Clustering. Un objet de cette classe contiendra les attributs suivants :<br>\n",
    "<li>Un champ dendrogram_root - une référence vers la racine du dendrogramme</li>\n",
    "<li>Un champ cluster_id_list - une liste de listes de movies id, i.e. la liste des clusters</li>\n",
    "<li>Un champ outliers - une liste de movies id, existant uniquement si on choisi la méthode perso</li>\n",
    "<li>Un champ cluster_features - le DataFrame des films nettoyé sur lequel on apprend </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalCluster:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        '''This is the HierarchicalCluster class constructor. It takes no arguments, all it does is to\n",
    "        build an object. The 4 attributes will be initialized later on, when training the model.'''\n",
    "        \n",
    "        self.dendrogram_root = None\n",
    "        self.cluster_id_list = None\n",
    "        self.outliers = None\n",
    "        self.cluster_features = None\n",
    "    \n",
    "    def set_cluster_features(self, dfm):\n",
    "        \n",
    "        '''This method is a setter for the attribute cluster_features. It takes one parameter, dfm, which is\n",
    "        a DataFrame that contains all the information about movies. The method cleans the DataFrame and keeps\n",
    "        only the attributes that are relevant for the movie clustering.'''\n",
    "        \n",
    "        # selection des attributs qui nous interessent pour le clustering\n",
    "        clu_fea = dfm[['genres','keywords','release_date','production_countries','original_language','runtime']]\n",
    "        # on met en index les id des movies\n",
    "        clu_fea.index = dfm.movieId.apply(lambda x: str(x))\n",
    "        # nettoyage du dataframe\n",
    "        clu_fea = clean_runtime(clu_fea)\n",
    "        clu_fea = drop_missing_values(clu_fea)\n",
    "        vectorize_keywords(clu_fea)\n",
    "        vectorize_genres(clu_fea)\n",
    "        simplify_date(clu_fea)\n",
    "        simplify_countries(clu_fea)\n",
    "        self.cluster_features = clu_fea\n",
    "    \n",
    "    def agglomerative_cluster(self, dist_mat):\n",
    "        \n",
    "        '''This method builds a dendrogram based on the distance matrix given in parameters. It returns\n",
    "        its root. Important things to know about this function : 1) In order to avoid redundancy and to make\n",
    "        the computations faster, it reduces the matrix size at each step by dropping a row and a colums.\n",
    "        At the end, the matrix has 1x1 shape, so if one wants to store the matrix and keep it unchanged,\n",
    "        he must call this method with a copy. 2) A choice which has a huge impact on the dendrogram was made\n",
    "        here. In the algorithm, the 2 closest clusters are merged into a bigger one at each step. But there are\n",
    "        several ways to measure the distance between clusters. It can be the distance between the centroids, the\n",
    "        average distance, the minimal or maximal distance bewteen 2 points from different clusters. We chose this\n",
    "        last option. It avoids to have many unbalanced branches, especially at levels close to the root.'''\n",
    "        \n",
    "        assert self.cluster_features is not None\n",
    "        clu_fea = self.cluster_features\n",
    "        clu_fea[\"dendrogram\"] = clu_fea.index\n",
    "        clu_fea.dendrogram = clu_fea.dendrogram.apply(lambda x: Dendrogram(leaf=int(x)))\n",
    "        size_mat = len(clu_fea)\n",
    "        for _ in range(1, size_mat):\n",
    "            # localisation de la plus petite distance dans la matrice\n",
    "            index_str1, index_str2 = dist_mat.stack().idxmin()\n",
    "            height = dist_mat.loc[index_str1, index_str2]\n",
    "            mov1 = clu_fea.loc[index_str1]\n",
    "            mov2 = clu_fea.loc[index_str2]\n",
    "            tmp1 = mov1.dendrogram\n",
    "            tmp2 = mov2.dendrogram\n",
    "            # acces a la racine du cluster de mov1 et de celui de mov2\n",
    "            while tmp1.father is not None: tmp1 = tmp1.father\n",
    "            while tmp2.father is not None: tmp2 = tmp2.father\n",
    "            # creation de la racine du nouvel arbre, fusion des 2 clusters\n",
    "            tmp3 = Dendrogram()\n",
    "            tmp3.left = tmp1\n",
    "            tmp3.right = tmp2\n",
    "            tmp1.father = tmp3\n",
    "            tmp2.father = tmp3\n",
    "            tmp3.set_leaf_nb()\n",
    "            tmp3.set_height(height)\n",
    "            # actualisation de la matrice de distance\n",
    "            new_d = np.maximum(dist_mat.loc[index_str1, :], dist_mat.loc[index_str2, :])\n",
    "            dist_mat.loc[index_str1, :] = dist_mat.loc[:, index_str1] = new_d\n",
    "            # suppression d'une des 2 lignes et colonnes qui font maintenant doublons\n",
    "            dist_mat = dist_mat.drop(index_str2, axis=0)\n",
    "            dist_mat = dist_mat.drop(index_str2, axis=1)\n",
    "        \n",
    "        return clu_fea.iloc[0].dendrogram.get_root()\n",
    "    \n",
    "    def train(self, dfm_cluster):\n",
    "        \n",
    "        '''This method coordinates the cluster construction. Firstly it cleans the DataFrame by calling the\n",
    "        set_cluster_features method. Then it calls the function that computes the distance matrix. Finally\n",
    "        it calls the agglomerative_cluster method in order to build the dendrogram.'''\n",
    "        \n",
    "        self.set_cluster_features(dfm_cluster)\n",
    "        dist_mat = compute_dist_matrix(self.cluster_features)\n",
    "        self.dendrogram_root = self.agglomerative_cluster(dist_mat)\n",
    "    \n",
    "    def set_n_clusters(self, n):\n",
    "        \n",
    "        '''This method cuts the dendrogram at a height that forms n different clusters.'''\n",
    "        \n",
    "        if self.dendrogram_root is None:\n",
    "            print(\"Erreur, vous devez d'abord construire le dendrogram avec la methode train.\")\n",
    "        else:\n",
    "            self.outliers = None\n",
    "            self.cluster_id_list = self.dendrogram_root.get_n_clusters(n)\n",
    "    \n",
    "    def set_best_n_clusters(self):\n",
    "        \n",
    "        '''This method cuts the dendrogram at a height that maximizes the distance_to_father attributes\n",
    "        of the cluster roots, on the condition that it doesn't create too many clusters.'''\n",
    "        \n",
    "        if self.dendrogram_root is None:\n",
    "            print(\"Erreur, vous devez d'abord construire le dendrogram avec la methode train.\")\n",
    "        else:\n",
    "            self.cluster_id_list = self.dendrogram_root.find_best_cut()\n",
    "    \n",
    "    def set_n_clusters_perso(self, n):\n",
    "        \n",
    "        '''This method cuts the dendrogram at different heights depending on the branch, so that if forms\n",
    "        n clusters quite well balanced. It does not take the nodes height in consideration.'''\n",
    "        \n",
    "        if self.dendrogram_root is None:\n",
    "            print(\"Erreur, vous devez d'abord construire le dendrogram avec la methode train.\")\n",
    "        else:\n",
    "            self.cluster_id_list, self.outliers = self.dendrogram_root.get_n_clusters_perso(n)\n",
    "    \n",
    "    def get_cluster(self, pos, movies=None):\n",
    "        \n",
    "        '''This method returns a DataFrame that contains the movies from cluster number pos (first pos is 1).\n",
    "        It uses the self.cluster_features attribute if the argument movies is not precised.'''\n",
    "        \n",
    "        if movies is None:\n",
    "            movies = self.cluster_features\n",
    "        if pos <= 0:\n",
    "            print(\"Erreur, l'argument doit etre >= 1\")\n",
    "            return\n",
    "        if pos > len(self.cluster_id_list):\n",
    "            print(\"Erreur, il y a seulement \", len(self.cluster_id_list), \" clusters.\")\n",
    "            return\n",
    "        if self.cluster_id_list is None:\n",
    "            print(\"Erreur, vous devez d'abord choisir un nombre de clusters avec la methode set_n_clusters.\")\n",
    "            return\n",
    "        df = pd.DataFrame([])\n",
    "        if 'movieId' in movies.columns:\n",
    "            for i in self.cluster_id_list[pos - 1]:\n",
    "                df = df.append(movies[movies.movieId == int(i)])\n",
    "        else:\n",
    "            for i in self.cluster_id_list[pos - 1]:\n",
    "                df = df.append(movies.loc[str(i)])\n",
    "        return df\n",
    "    \n",
    "    def get_outliers(self, movies=None):\n",
    "        \n",
    "        '''This method returns a DataFrame that contains all the outliers movies. It exists only if the perso\n",
    "        method is used when setting a number of clusters. Otherwise it returns an empty DataFrame.'''\n",
    "        \n",
    "        df = pd.DataFrame([])\n",
    "        if self.outliers is None:\n",
    "            return df\n",
    "        if movies is None:\n",
    "            movies = self.cluster_features\n",
    "        if 'movieId' in movies.columns:\n",
    "            for i in outliers:\n",
    "                df = df.append(movies[movies.movieId == int(i)])\n",
    "        else:\n",
    "            for i in outliers:\n",
    "                df = df.append(movies.loc[str(i)])\n",
    "        return df\n",
    "    \n",
    "    def get_clusters_size(self):\n",
    "        \n",
    "        '''This method returns a list containing the size of the clusters,\n",
    "        i.e. the number of movies for each cluster.'''\n",
    "        \n",
    "        if self.cluster_id_list is None:\n",
    "            print(\"Erreur, vous devez d'abord choisir un nombre de clusters avec la methode set_n_clusters.\")\n",
    "            return\n",
    "        l = []\n",
    "        for id_list in self.cluster_id_list:\n",
    "            l.append(len(id_list))\n",
    "        return l\n",
    "    \n",
    "    def find_closest_cluster(self, movie):\n",
    "        \n",
    "        '''This method finds the closest cluster for a movie given in parameter. The closest cluster is the\n",
    "        cluster that contains the movie that is the closest to the argument.'''\n",
    "        \n",
    "        if self.cluster_id_list is None:\n",
    "            print(\"Erreur, vous devez d'abord choisir un nombre de clusters avec la methode set_n_clusters.\")\n",
    "            return\n",
    "        min_list = []\n",
    "        for cluster in self.cluster_id_list:\n",
    "            min_dist = np.min([movie_distance(movie, self.cluster_features.loc[str(m)]) for m in cluster])\n",
    "            min_list.append(min_dist)\n",
    "        index_closest_cluser = np.argmin(min_list)\n",
    "        \n",
    "        return self.cluster_id_list[index_closest_cluser]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sur un échantillon de taille 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10 = HierarchicalCluster()\n",
    "test_10 = dfm_cluster.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cluster_10.train(test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.set_n_clusters(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(1, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(2, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(3, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.set_n_clusters_perso(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(1, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(2, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(3, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.set_best_n_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(1, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(2, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(3, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(4, cluster_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_10.get_cluster(5, cluster_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sur un échantillon de taille 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_100 = dfm_cluster.sample(100)\n",
    "cluster_100 = HierarchicalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cluster_100.train(test_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f movie_distance cluster_100.train(test_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_100.set_n_clusters(6)\n",
    "cluster_100.get_clusters_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_100.set_n_clusters_perso(6)\n",
    "print(len(cluster_100.outliers), \" outliers\")\n",
    "cluster_100.get_clusters_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_100.set_best_n_clusters()\n",
    "cluster_100.get_clusters_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Données complètes : 119 minutes pour run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = HierarchicalCluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time cluster.train(dfm_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.set_n_clusters(10)\n",
    "cluster.get_clusters_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.set_n_clusters_perso(10)\n",
    "print(len(cluster_100.outliers), \" outliers\")\n",
    "cluster.get_clusters_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.set_best_n_clusters()\n",
    "cluster.get_clusters_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybride Cluster-Collabo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERER DE LA DOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryClusterRecommander:\n",
    "    def __init__(self,base):\n",
    "        self.memory = MemoryBasedPredictor(base)\n",
    "        self.cluster = None\n",
    "    \n",
    "    def fit(self, movies, ratings, link, cor_fct, verbose=False):\n",
    "        self.cluster = HierarchicalCluster()\n",
    "        \n",
    "        self.memory.fit(ratings, cor_fonct, verbose)\n",
    "    \n",
    "    \n",
    "    ###COPIER COLLER VIOLENT DE CE QU'ON AVAIT FAIT AVEC MATHILDE\n",
    "    \n",
    "    def find_cluster( movieId) :#l'utilisateur uid veut une liste de films qu'il aimerait et ressemblent à movieId\n",
    "        movieId = str(movieId)\n",
    "        for i in range(1,len(cluster.get_clusters_size())) :\n",
    "            if movieId in cluster.get_cluster(i).index :\n",
    "                return cluster.get_cluster(i)\n",
    "        return (\"Le film n'existe pas\")\n",
    "    \n",
    "    def recommend(uid,mid,nb_reco=10,base=\"user\",k=4):\n",
    "        '''\n",
    "        Suggère à l'utilisateur uid le top nb_reco de films qui ressemblent au film mid (car dans le même cluster)\n",
    "        qu'il est le plus susceptible d'apprécier (en trouvant les notes grâce à predict_collabo)\n",
    "        '''\n",
    "        predictions = pd.DataFrame(columns=['movieId', 'predict_rating'])\n",
    "        \n",
    "        cluster = find_cluster(mid)\n",
    "        for movie in cluster.index :\n",
    "            p, o = (uid, movie) if base == 'user' else (movie, uid)\n",
    "            rat = cluster.predict(p, o, base, k)\n",
    "            predictions = predictions.append({'movieId':int(movie), 'predict_rating':rat}, ignore_index=True)\n",
    "       \n",
    "        predictions = predictions.sort_values(by='predict_rating', ascending=False)\n",
    "        reco = cluster.merge(reco, how='inner')\n",
    "        reco = predictions.head(nb_reco) if predictions.shape[0] > nb_reco else predictions\n",
    "        return reco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice des notes user-item $R$ est partiellement vide. Ainsi réduire les dimensions de la matrice pourrait améliorer la complexité de nos algorithmes. Une méthode que nous pourrions avoir envie d'utiliser est la décomposision en valeurs singulières : $R = U_{svd} \\Sigma V_{svd}$. Cependant cette méthode ne s'applique pas ici étant donné que $R$ n'est pas complète et qu'on a besoin de réaliser des calculs algébriques avec $R$ pour trouver la décomposition.\n",
    "\n",
    "On considère donc un modèle dans lequel il existe des attributs décrivants les films et les préférences des utilisateurs. La matrice $R$ peut alors être factorisée en produit de deux matrices $U$ et $V$ représentant respectivement les utilisateurs et les items :\n",
    "\n",
    "$$\n",
    "R \\approx U \\times V^T\n",
    "$$\n",
    "\n",
    "avec $R \\in \\mathbb{R}^{n \\times m}$ la matrice des notes user-item, $U \\in \\mathbb{R}^{n \\times \\ell}$ la matrice des users, $V \\in \\mathbb{R}^{m \\times \\ell}$ la matrice des items et $\\ell$ le nombre d'attributs. Pour faire un rapprochement avec la SVD, on peut considerer que $U = U_{svd} \\Sigma^{1/2}$ et $V = \\Sigma^{1/2} V_{svd}$. On note $U_i$ les lignes de $U$ et $V_j$ les lignes de $V$ :\n",
    "$\n",
    "U = \\left[ \\begin{array}{c} U_1 \\\\ \\vdots \\\\ U_n \\end{array} \\right]\n",
    "$ et \n",
    "$\n",
    "V = \\left[ \\begin{array}{c} V_1 \\\\ \\vdots \\\\ V_m \\end{array} \\right]\n",
    "$\n",
    "avec $U_i^T, V_j^T \\in \\mathbb{R^\\ell}$.\n",
    "\n",
    "Dans ce modèle, chaque note $R_{ij}$ associée à un couple user-item $(i, j)$ est le résultat du produit scalaire entre la ligne associée au user $i$ dans $U$ et la ligne associée au item $j$ dans $V$ : $R_{ij} = U_i \\cdot V_j^T$. Une fois les matrices $U$ et $V$ apprises, pour prédire une note il suffira de faire le produit scalaire entre les lignes associées.\n",
    "\n",
    "Trouver $U$ et $I$ revient à minimiser l'erreur entre la note prédite $U_i \\cdot V_j^T$ et la véritable note $R_{ij}$. Il s'agit du problème de minimisation suivant, avec $E = \\{(i, j) \\mbox{ | } R_{ij} \\mbox{ connue}\\}$ :\n",
    "\n",
    "$$\n",
    "(U, V) = argmin_{(U, V)} \\sum_{(i, j) \\in E} [U_i \\cdot V_j^T - R_{ij}]^2\n",
    "$$\n",
    "\n",
    "qui est équivalent à:\n",
    "\n",
    "$$\n",
    "(U, V) = argmin_{(U, V)} \\frac{1}{2}\\sum_{(i, j) \\in E} [U_i \\cdot V_j^T - R_{ij}]^2 + \\lambda (\\|U_i\\|^2 + \\|V_j\\|^2)\n",
    "$$\n",
    "\n",
    "Le terme de droite est un terme régulateur, de paramètre $\\lambda$ à ajuster, permettant de prévenir un overfitting.\n",
    "\n",
    "Pour résoudre ce problème, nous allons utiliser une méthode de descente de gradient.\n",
    "\n",
    "\n",
    "*Pour résoudre ce problème, on peut utiliser une méthode de descente de gradient. Nous allons ensuite optimiser cette méthode en utilisant d'abord des batch, puis en se réduisant à un problème de moindre carré en fixant alternativement les matrices $U$ et $V$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente de gradient (à pas constant)\n",
    "\n",
    "Dans notre [cours d'optimisation](https://www.ceremade.dauphine.fr/~gontier/enseignement.html) donné par David Gontier, nous avons étudié différentes méthodes de descente de gradient de complexité et d'optimalité différentes. Cependant il nous semble qu'utiliser une version simple à pas $\\tau$ constant suffit. Il sera possible de régler cet hyper-paramètre par validation croisée. \n",
    "\n",
    "Notre fonction objective est la suivante :\n",
    "$$\n",
    "F(U, V) := \\sum_{(i, j) \\in E} \\frac{1}{2}[U_i \\cdot V_j^T - R_{ij}]^2 + \\frac{\\lambda}{2} (\\|U_i\\|^2 + \\|V_j\\|^2)\n",
    "$$\n",
    "\n",
    "Dans une descente de gradient classique, à chaque itération on met à jour $U$ et $V$ suivant la formule \n",
    "$\n",
    "(U, V) = (U, V) - \\tau \\nabla F(U, V)\n",
    "$. Cependant, dans notre cas nous n'allons pas mettre à jour toutes les lignes de $U$ et $V$ simultanément. En effet, puisque la somme dans $F$ ne se fait que sur les couples $(i, j)$ pour lesquels la note est connue, nous allons seulement mettre à jour le couple $(U_i, V_j)$ associé en itérant sur tous les couples $(i, j) \\in E$. \n",
    "\n",
    "Pour une note $R_{ij}$, on a \n",
    "$\n",
    "\\frac{\\partial F}{\\partial U_i} = V_j^T (U_i \\cdot V_j^T - R_{ij}) + \\lambda U_i\n",
    "$\n",
    " et \n",
    "$\n",
    "\\frac{\\partial F}{\\partial V_j} = Ui (U_i \\cdot V_j^T - R_{ij}) + \\lambda V_j\n",
    "$\n",
    "donc on peut mettre à jour les lignes $U_i$ et $V_j$ selon les formules \n",
    "$$\n",
    "U_i = Ui - \\tau [V_j^T (U_i \\cdot V_j^T - R_{ij}) + \\lambda U_i]\\\\\n",
    "V_j = V_j - \\tau [Ui (U_i \\cdot V_j^T - R_{ij}) + \\lambda V_j]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il se peut que tous les entiers entre 1 et $n$ (ou $m$) ne soient pas utlisés par les id des users (ou des movies). Ceci est par exemple le cas lorsqu'on travaille avec un échantillon des données. Puisque nous aimerions utiliser des numpy array dans nos calculs, il va être nécessaire d'avoir la correspondant entre les id et les indices utilisés dans les numpy array (que nous allons appeler rang). Pour cela, utilisons simplement une liste contenant les id et dont l'indice dans la liste d'un id donné correspondra au rang. Pour trouver l'id à partir d'un rang il suffira de faire un simple extraction, pour trouver le rang à partir d'un id on utilisera la méthode `index()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice $R$ étant vide, nous n'allons pas utiliser de matrice pour la représenter et garderons la dataframe qui ne contient que les notes connues. Nous allons également avoir besoin d'écrire une fonction `get_rat()` qui permet d'accéder à la note d'un couple de rang dans la dataframe des notes. Nous utilisons également une fonction `known()` pour construire l'ensemble $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent écrire la fonction résolvant notre problème de minimisation. Remarquons qu'elle modifie les valeurs de $U$ et $V$ en place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedPredictor:\n",
    "    def __init__(self):\n",
    "        self.R = None\n",
    "        self.E = None\n",
    "        self.U = None\n",
    "        self.V = None\n",
    "        self.user_id = None\n",
    "        self.movie_id = None\n",
    "        self.user_rank = None\n",
    "        self.movie_rank = None\n",
    "    \n",
    "    def fit(self, ratings, ell=10, lamb=1/2, tau=1/10, tol=1e-3, Niter=1000, verbose=False):\n",
    "        if verbose:\n",
    "            logger = logging.getLogger()\n",
    "            logger.setLevel(logging.INFO)\n",
    "        \n",
    "        dfr = ratings[['userId', 'movieId', 'rating']]\n",
    "\n",
    "        # correspondance rang - id\n",
    "        self.user_id = dfr['userId'].unique().tolist()\n",
    "        self.movie_id = dfr['movieId'].unique().tolist()\n",
    "        self.user_id.sort()\n",
    "        self.movie_id.sort()\n",
    "        # correspondance id - rang\n",
    "        self.user_rank, self.movie_rank = {}, {}\n",
    "        for uid in self.user_id:\n",
    "            self.user_rank[uid] = len(self.user_rank)\n",
    "        for mid in self.movie_id:\n",
    "            self.movie_rank[mid] = len(self.movie_rank)\n",
    "\n",
    "        # transformer la dataframe en matric numpy\n",
    "        dfr.set_index(['userId', 'movieId'], inplace=True)\n",
    "        self.R = dfr.unstack(level='movieId').to_numpy()\n",
    "\n",
    "        # construction de l'ensemble E, ensemble des rangs pour lesquels R[i, j] est connu\n",
    "        ids = list(dfr.index)\n",
    "        self.E = list(map(lambda t : (self.user_rank[t[0]], self.movie_rank[t[1]]), ids))\n",
    "\n",
    "        n = len(self.user_rank)\n",
    "        m = len(self.movie_rank)\n",
    "\n",
    "        self.U, self.V = np.random.rand(n, ell), np.random.rand(m, ell)\n",
    "        self._descenteGradient_(lamb, tau, tol, Niter, verbose)\n",
    "    \n",
    "    def fit_linear_approach(self, ratings, clu_fea, lamb=1/2, tau=1/10, tol=1e-3, Niter=1000):\n",
    "        \n",
    "        dfr = ratings[['userId', 'movieId', 'rating']]\n",
    "\n",
    "        # correspondance rang - id\n",
    "        self.user_id = dfr['userId'].unique().tolist()\n",
    "        self.movie_id = dfr['movieId'].unique().tolist()\n",
    "        self.user_id.sort()\n",
    "        self.movie_id.sort()\n",
    "        # correspondance id - rang\n",
    "        self.user_rank, self.movie_rank = {}, {}\n",
    "        for uid in self.user_id:\n",
    "            self.user_rank[uid] = len(self.user_rank)\n",
    "        for mid in self.movie_id:\n",
    "            self.movie_rank[mid] = len(self.movie_rank)\n",
    "\n",
    "        # transformer la dataframe en matric numpy\n",
    "        dfr.set_index(['userId', 'movieId'], inplace=True)\n",
    "        self.R = dfr.unstack(level='movieId').to_numpy()\n",
    "        \n",
    "        # construction de l'ensemble E, ensemble des rangs pour lesquels R[i, j] est connu\n",
    "        ids = list(dfr.index)\n",
    "        self.E = list(map(lambda t : (self.user_rank[t[0]], self.movie_rank[t[1]]), ids))\n",
    "        \n",
    "        # construction de la matrice V a partir du DataFrame clu_fea\n",
    "        V = select_useful_features(clu_fea)\n",
    "        V = normalize_runtime(V)\n",
    "        V = normalize_release_date(V)\n",
    "        V = normalize_lang(V)\n",
    "        V = normalize_genres(V)\n",
    "        V = normalize_prod_countries(V)\n",
    "        self.V = V\n",
    "        ell = V.shape[1]\n",
    "        n = len(self.user_rank)\n",
    "        self.U = np.random.rand(n, ell)\n",
    "        \n",
    "        self._descenteGradient_(lamb, tau, tol, Niter, linear=True)\n",
    "        \n",
    "    def predict(self, uid, mid):\n",
    "        assert self.E is not None, \"This ModelBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        ell = self.U.shape[1]\n",
    "        \n",
    "        u = self.U[self.user_rank[uid]] if uid in self.user_rank else np.random.rand(ell)\n",
    "        v = self.V[self.movie_rank[mid]] if mid in self.movie_rank else np.random.rand(ell)\n",
    "        \n",
    "\n",
    "        pred = np.dot(u, v.T)\n",
    "        # note prédite à une précision de 0.5\n",
    "        pred = round(2 * pred) / 2\n",
    "        # note prédite se trouve entre 0 et 5\n",
    "        pred = 5 if pred > 5 else pred\n",
    "        pred = 0 if pred < 0 else pred\n",
    "        return pred\n",
    "\n",
    "    \n",
    "    def score(self, test_ratings):\n",
    "        assert self.E is not None, \"This ModelBasedPredictor instance is not fitted yet. \\\n",
    "                                                Call 'fit' with appropriate arguments before using this estimator.\"\n",
    "        \n",
    "        dft = test_ratings.set_index(['userId', 'movieId'])\n",
    "        predictions = [self.predict(uid, mid) for (uid, mid) in list(dft.index)]\n",
    "        truth = dft['rating'].to_numpy()\n",
    "    \n",
    "        return 1 - math.sqrt(sum((predictions - truth)**2) / len(predictions) ) / 5\n",
    "             \n",
    "    def _norm2_(self, X):\n",
    "        return sum([X[i]**2 for i in range(len(X))])\n",
    "\n",
    "    def _F_(self, lamb):\n",
    "        return sum([1/2 * (np.dot(self.U[i], self.V[j].T) - self.R[i, j])**2 \\\n",
    "                    + lamb/2 * (self._norm2_(self.U[i]) + self._norm2_(self.V[j])) for (i, j) in self.E])\n",
    "    \n",
    "    def _F_linear_(self, lamb):\n",
    "        return sum([1/2 * (np.dot(self.U[i], self.V.iloc[j]) - self.R[i, j])**2 \\\n",
    "                    + lamb/2 * self._norm2_(self.U[i]) for (i, j) in self.E])\n",
    "    \n",
    "    def _descenteGradient_(self, lamb, tau, tol=1e-3, Niter=1000, verbose=False, linear=False):\n",
    "        R = self.R\n",
    "        E = self.E\n",
    "        U, V = self.U, self.V\n",
    "        \n",
    "        if verbose:\n",
    "            logging.info('nombre de couples : {}'.format(len(E)))\n",
    "            logging.info(\"nombre d'iteration max : {}\".format(Niter))\n",
    "            \n",
    "        last_F = 0\n",
    "        for n in range(Niter):\n",
    "            \n",
    "            new_F = self._F_linear_(lamb) if linear else self._F_(lamb)\n",
    "\n",
    "            if verbose :\n",
    "                logging.info(\"{}. pente de F: {}\".format(n, abs(new_F - last_F)))\n",
    "\n",
    "            if abs(new_F - last_F) < tol:\n",
    "                return\n",
    "            last_F = new_F   \n",
    "\n",
    "            for (i, j) in E:\n",
    "                gradU = V.iloc[j].T * (np.dot(U[i], V.iloc[j].T) - R[i, j]) + lamb * U[i]\n",
    "                U[i] -= tau * gradU\n",
    "                if not linear:\n",
    "                    gradV = U[i] * (np.dot(U[i], V[j].T) - R[i, j]) + lamb * V[j]\n",
    "                    V[j] -= tau * gradV\n",
    "        print(\"Erreur, l’algorithme n’a pas convergé après\", Niter ,\" itérations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(df, train_prop):\n",
    "    '''\n",
    "    Sépare la dataframe df en deux dataframe train et test.\n",
    "    :param: df la dataframe à séparer\n",
    "            train_prop la proportion d'element qu'il doit y avoir dans le train dataframe\n",
    "            seed la graine du générateur aléatoire\n",
    "    :return: les train et test dataframes\n",
    "    '''\n",
    "    train = df.sample(frac=train_prop)\n",
    "    test = df.drop(train.index)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(df, nb_folds):\n",
    "    '''\n",
    "    Sépare la dataframe df en nb_folds dataframe de taille (presque) égales.\n",
    "    :param: df la dataframe à séparer\n",
    "            nb_folds le nombre d'échantillons à consitutuer à partir de df\n",
    "    :return: la liste des dataframes\n",
    "    '''\n",
    "    N = len(df)\n",
    "    f_size = N // nb_folds\n",
    "    folds = [None] * nb_folds\n",
    "    for i in range(nb_folds - 1):\n",
    "        folds[i] = df.sample(n=f_size)\n",
    "        df = df.drop(folds[i].index)\n",
    "    folds[nb_folds - 1] = df\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(predictor, df, params, nb_folds):\n",
    "    \n",
    "    # verification que les arguments donnés dans le dictionnaire params correspondent bien aux paramètre de predictor\n",
    "    var = getattr(getattr(predictor.fit, '__code__'), 'co_varnames')\n",
    "    assert set(params.keys()) <= set(var[1:]), 'invalid arguments given in params' # var[1:] pour enlever 'self'\n",
    "    \n",
    "    \n",
    "    folds = k_fold(df, nb_folds)\n",
    "    scores = [None] * nb_folds\n",
    "    \n",
    "    for i in range(nb_folds):\n",
    "        test = folds[i]\n",
    "        train = df.drop(test.index)\n",
    "        \n",
    "        pred = predictor()\n",
    "        pred.fit(train, **params)\n",
    "        scores[i] = pred.score(test)\n",
    "    \n",
    "    return sum(scores) / nb_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinaisons(grid):\n",
    "    for v in product(*grid.values()):\n",
    "        yield {id:v[i] for i, id in enumerate(grid.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(predictor, df, param_grid, cv, verbose=False):\n",
    "    best_score = 0\n",
    "    best_param = None\n",
    "    for param in combinaisons(param_grid):\n",
    "        score = cross_validation(predictor, df, param, cv)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_param = param\n",
    "        \n",
    "        if verbose:\n",
    "            print('CV score: {} for param: {}'.format(score, param))\n",
    "\n",
    "    return best_score, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_linear = movies.join(link.set_index('tmdbId'), on='tmdbId', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_linear = dfm_linear.merge(ratings.drop_duplicates('movieId'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_linear = ratings.merge(dfm_linear.movieId, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ModelBasedPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f ModelBasedPredictor._descenteGradient_ test.fit_linear_approach(ratings_linear, dfm_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model : content-based\n",
    "\n",
    "On remarque que si $U$ ou $V$ est fixé, la fonction objective devient quadratique. Or nous connaissons des algorithmes efficaces pour minimiser des fonctions quadratiques. De plus, une matrice d'attributs des films peut être donnée puisqu'on connaît certaines informations sur les films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_useful_features(V):\n",
    "    return V[['genres', 'release_date', 'production_countries', 'original_language', 'runtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = select_useful_features(dfm_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_runtime(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    min_runtime = min(dfm.runtime)\n",
    "    max_runtime_diff = max(dfm.runtime) - min_runtime\n",
    "    if max_runtime_diff <= 1:\n",
    "        return dfm\n",
    "    dfm.runtime = dfm.runtime.apply(lambda x: (x - min_runtime) / max_runtime_diff)\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_runtime(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_release_date(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    simplify_date(dfm)\n",
    "    min_date = min(dfm.release_date)\n",
    "    max_date_diff = max(dfm.release_date) - min_date\n",
    "    if max_date_diff <= 1:\n",
    "        return dfm\n",
    "    dfm.release_date = dfm.release_date.apply(lambda x: (x - min_date) / max_date_diff)\n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_release_date(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lang(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    if 'original_language' not in dfm.columns:\n",
    "        return dfm\n",
    "    languages = np.unique(dfm.original_language)\n",
    "    nb_lang = len(languages)\n",
    "    for lang in languages:\n",
    "        dfm[lang] = pd.Series()\n",
    "        dfm[lang] = dfm[lang].fillna(0)\n",
    "    for index, lang in dfm.original_language.iteritems():\n",
    "        dfm.at[index, lang] = 1\n",
    "    dfm = dfm.drop('original_language', axis=1)\n",
    "    \n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_lang(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_genres(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    if 'genres' not in dfm.columns:\n",
    "        return dfm\n",
    "    vectorize_genres(dfm)\n",
    "    genres_list = np.array([])\n",
    "    for _, tab in dfm.genres.iteritems():\n",
    "        genres_list = np.unique(np.append(genres_list, tab))\n",
    "    for genre_id in genres_list:\n",
    "        col_name = \"genId\" + str(int(genre_id))\n",
    "        dfm[col_name] = pd.Series()\n",
    "        dfm[col_name] = dfm[col_name].fillna(0)\n",
    "    for index, gen in dfm.genres.iteritems():\n",
    "        for g in gen:\n",
    "            dfm.at[index, \"genId\" + str(g)] = 1 / len(gen)\n",
    "    dfm = dfm.drop('genres', axis=1)\n",
    "    \n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_genres(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prod_countries(dfm_arg):\n",
    "    dfm = dfm_arg.copy()\n",
    "    if 'production_countries' not in dfm:\n",
    "        return dfm\n",
    "    simplify_countries(dfm)\n",
    "    country_list = np.array([])\n",
    "    for _, tab in dfm.production_countries.iteritems():\n",
    "        country_list = np.unique(np.append(country_list, tab))\n",
    "    for country in country_list:\n",
    "        dfm[country] = pd.Series()\n",
    "        dfm[country] = dfm[country].fillna(0)\n",
    "    for index, countries in dfm.production_countries.iteritems():\n",
    "        for c in countries:\n",
    "            dfm.at[index, c] = 1 / len(countries)\n",
    "    dfm = dfm.drop('production_countries', axis=1)\n",
    "    \n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = normalize_prod_countries(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
